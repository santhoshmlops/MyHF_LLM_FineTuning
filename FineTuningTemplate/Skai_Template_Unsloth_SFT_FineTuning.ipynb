{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning with Unsloth - SFT Template"
      ],
      "metadata": {
        "id": "TQJhp1_exyZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "Unsloth is an open-source platform for fine-tuning popular Large Language Models faster. It supports popular LLMs, including Llama-2 and Mistral, and their derivatives like Yi, Open-hermes, etc. It implements custom triton kernels and a manual back-prop engine to improve the speed of the model training.\n",
        "\n",
        "Here, we will use the Unsloth to Fine-tune a base 4-bit quantized Tiny-Llama model on the Alpaca dataset. The model is quantized with bits and bytes, and kernels are optimized with OpenAIâ€™s Triton.\n",
        "\n",
        "`I prepared this Fine-Tuning with Unsloth - SFT Template for my use case, but you could change it to suit your requirements.`\n",
        "\n",
        "\n",
        "\n",
        "To View My Account:\n",
        "\n",
        "* [Hugging Face ](https://huggingface.co/santhoshmlops)\n",
        "\n",
        "* [Git Hub](https://github.com/santhoshmlops)\n",
        "\n",
        "To View Some other Fine Tuning Template:\n",
        "\n",
        "* [Fine Tuning Template ](https://github.com/santhoshmlops/MyHF_LLM_FineTuning/tree/main/FineTuningTemplate)\n",
        "\n",
        "\n",
        "To View My Model Fine Tuning  NoteBook:\n",
        "\n",
        "* [MY HF LLM Fine-Tuning](https://github.com/santhoshmlops/MyHF_LLM_FineTuning)\n",
        "\n"
      ],
      "metadata": {
        "id": "EccxiIzQyHje"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up on Google Colab\n",
        "Google Colab provides a convenient, cloud-based environment with access to powerful GPUs like the `T4`. If you choose Colab for this tutorial, make sure to select a GPU runtime by going to `Runtime > Change runtime type > T4 GPU`. This ensures that your notebook has access to the necessary computational resources."
      ],
      "metadata": {
        "id": "Rj8jNEOWyTZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up Hugging Face Authentication\n",
        "\n",
        "On Google Colab, you can safely store your Hugging Face token by using Colab's \"Secrets\" feature. This can be done by clicking on the \"Key\" icon in the sidebar, selecting \"`Secrets`\", and adding a new secret with the name `HF_TOKEN` and your Hugging Face token as the value. This method ensures that your token remains secure and is not exposed in your notebook's code."
      ],
      "metadata": {
        "id": "w9CQq_XDyYAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1 - Install the required Python packages"
      ],
      "metadata": {
        "id": "a-Ei9GrHyyZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"unsloth[colab-ampere] @ git+https://github.com/unslothai/unsloth.git\""
      ],
      "metadata": {
        "id": "Loxa-902yU6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2 - Logging into Hugging Face Hub\n",
        "Paste the Hugging Face Hub Write API KEY"
      ],
      "metadata": {
        "id": "ugBIiGYfy3F6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLZkfGTFxrbr"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3 - Loading Required Libraries"
      ],
      "metadata": {
        "id": "x2ZZvg0My8h6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "5HcFxwQFy-oV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4 - Setting Model Parameters for SFT\n",
        "`Note:` The parameter can be changed for fine tuning, or it can be left as it is and filled with the value of the empty parameter."
      ],
      "metadata": {
        "id": "VOdfahrNzBfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Model for Tuning\n",
        "model_ckpt = \"\"                                                             # Change the model_ckpt as your wish.\n",
        "hf_user_name = \"\"                                                           # Change the user_name as your wish.\n",
        "hub_model_ckpt = hf_user_name+\"/\"+ model_ckpt.split(\"/\")[-1]+\"-Unsloth-SFT\" # Change the hub_model_ckpt as your wish.\n",
        "dataset_name = \"databricks/databricks-dolly-15k\"\n",
        "\n",
        "# Automodel/Tokenizer Parameters\n",
        "max_seq_length = 1024         # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None                  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True           # Use for 4bit quantization\n",
        "\n",
        "# Lora Parameters\n",
        "r = 8                         # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\"]\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.05           # Supports any, but = 0 is optimized\n",
        "bias = \"none\"                 # Supports any, but = \"none\" is optimized\n",
        "use_gradient_checkpointing = True\n",
        "use_rslora = False            # We support rank stabilized LoRA\n",
        "loftq_config = None           # And LoftQ\n",
        "\n",
        "# Training Parameters\n",
        "output_dir = model_ckpt.split(\"/\")[-1]+\"-Unsloth-SFT\"   # Change the model_ckpt as your wish.\n",
        "num_train_epochs = 1\n",
        "per_device_train_batch_size = 3\n",
        "gradient_accumulation_steps = 1\n",
        "gradient_checkpointing = True\n",
        "max_grad_norm = 0.3\n",
        "learning_rate = 2e-4\n",
        "weight_decay = 0.003\n",
        "optim = \"paged_adamw_32bit\"\n",
        "lr_scheduler_type = \"cosine\"\n",
        "max_steps = 250\n",
        "warmup_ratio = 0.03\n",
        "group_by_length = True\n",
        "save_steps = 50\n",
        "save_strategy = \"epoch\"\n",
        "logging_steps = 50\n",
        "logging_dir = \"./logs\"\n",
        "fp16 = False\n",
        "bf16 = False\n",
        "packing = True\n",
        "neftune_noise_alpha = 5\n",
        "device_map = \"auto\"\n",
        "report_to = \"tensorboard\"\n",
        "\n",
        "# SFT Training Parameters\n",
        "dataset_text_field = \"text\"\n",
        "dataset_num_proc = 2\n",
        "packing = True\n",
        "max_seq_length = 1024\n",
        "\n",
        "# Merge and push the model to Hub\n",
        "low_cpu_mem_usage = True\n",
        "return_dict = True"
      ],
      "metadata": {
        "id": "3jwmkLgszEDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5 - Load the Model and Tokenizer"
      ],
      "metadata": {
        "id": "0_THRVMPd_so"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model and tokenizer with specified configurations.\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_ckpt,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit\n",
        ")\n",
        "\n",
        "# Prepare the model with LoRA (Low-Rank Adaptation) configuration\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = r,\n",
        "    target_modules = target_modules,\n",
        "    lora_alpha = lora_alpha,\n",
        "    lora_dropout = lora_dropout,\n",
        "    bias = bias,\n",
        "    use_gradient_checkpointing = use_gradient_checkpointing,\n",
        ")"
      ],
      "metadata": {
        "id": "EwCrnBTQd-7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6 - Loading and Formatting the Dataset\n",
        "`Note:` Prepare your dataset for fine tuning by defining and formatting it for your use case. The `def create_data():` function is an example for tuning the dataset."
      ],
      "metadata": {
        "id": "2UAGVm3QzGdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Based on given instruction and context, generate an appropriate response\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Context:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\n",
        "\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    contexts = examples[\"context\"]\n",
        "    responses = examples[\"response\"]\n",
        "    texts = []\n",
        "\n",
        "    for i,j,k  in zip(instructions, contexts,responses):\n",
        "        text = prompt.format(i,j,k) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "\n",
        "dataset = load_dataset(dataset_name, split = \"train\")\n",
        "train_dataset = dataset.map(formatting_prompts_func, batched = True)\n",
        "print(train_dataset[0])"
      ],
      "metadata": {
        "id": "dqKm2Qt7BM8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7 - Fine-Tuning with Supervised Finetuning"
      ],
      "metadata": {
        "id": "0We6QTCPzK_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    neftune_noise_alpha = neftune_noise_alpha\n",
        ")\n",
        "\n",
        "# Create a trainer for training the model.\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    dataset_text_field = dataset_text_field,\n",
        "    max_seq_length = max_seq_length,\n",
        "    packing = packing,\n",
        "    args = training_arguments,\n",
        ")"
      ],
      "metadata": {
        "id": "ng1J_cShXLY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8 - Lets start the training process"
      ],
      "metadata": {
        "id": "2N9VTOBGzPTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model and save it.\n",
        "trainer.train()\n",
        "trainer.push_to_hub(hub_model_ckpt)\n",
        "tokenizer.push_to_hub(hub_model_ckpt)"
      ],
      "metadata": {
        "id": "jlaFWJ90zQ5M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}