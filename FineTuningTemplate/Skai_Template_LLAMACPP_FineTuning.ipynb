{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# llama.cpp Fine-Tuning Template"
      ],
      "metadata": {
        "id": "uB0nzpQxteff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "llama.cpp helps to enable LLM inference with minimal setup and state-of-the-art performance on a wide variety of hardware.This allows the use of models packaged as .gguf files, which run efficiently in CPU-only and mixed CPU/GPU environments\n",
        "\n",
        "`I prepared this llama.cpp fine-tuning template for my use case, but you could change it to suit your requirements.`\n",
        "\n",
        "\n",
        "\n",
        "To View My Account:\n",
        "\n",
        "* [Hugging Face ](https://huggingface.co/santhoshmlops)\n",
        "\n",
        "* [Git Hub](https://github.com/santhoshmlops)\n",
        "\n",
        "To View Some other Fine Tuning Template:\n",
        "\n",
        "* [Fine Tuning Template ](https://github.com/santhoshmlops/MyHF_LLM_FineTuning/FineTuningTemplate)\n",
        "\n",
        "\n",
        "To View My Model Fine Tuning  NoteBook:\n",
        "\n",
        "* [MY HF LLM Fine-Tuning](https://github.com/santhoshmlops/MyHF_LLM_FineTuning)\n",
        "\n"
      ],
      "metadata": {
        "id": "aTFzQJS5tC45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up on Google Colab\n",
        "Google Colab provides a convenient, cloud-based environment with access to powerful GPUs like the `T4`. If you choose Colab for this tutorial, make sure to select a GPU runtime by going to `Runtime > Change runtime type > T4 GPU`. This ensures that your notebook has access to the necessary computational resources."
      ],
      "metadata": {
        "id": "0maUK2u9tDGG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up Hugging Face Authentication\n",
        "\n",
        "On Google Colab, you can safely store your Hugging Face token by using Colab's \"Secrets\" feature. This can be done by clicking on the \"Key\" icon in the sidebar, selecting \"`Secrets`\", and adding a new secret with the name `HF_TOKEN` and your Hugging Face token as the value. This method ensures that your token remains secure and is not exposed in your notebook's code."
      ],
      "metadata": {
        "id": "U4qiNzd-tPus"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Things to change in this template"
      ],
      "metadata": {
        "id": "982vrmE4tpik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* Model Name\n",
        " - Check the llama.cpp Github page to find the Supported models before you proceed:  [Git Hub Page](https://github.com/ggerganov/llama.cpp)\n",
        "* Quantization Methods\n",
        "* Hugging Face User Name"
      ],
      "metadata": {
        "id": "kj1NT4KjAdgy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantization Methods\n",
        "\n",
        "The names of the quantization methods follow the naming convention: \"q\" + the number of bits + the variant used (detailed below). Here is a list of all the possible quant methods and their corresponding use cases, based on model cards made by [TheBloke](https://huggingface.co/TheBloke/):\n",
        "\n",
        "* `q2_k`: Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors.\n",
        "* `q3_k_l`: Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\n",
        "* `q3_k_m`: Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\n",
        "* `q3_k_s`: Uses Q3_K for all tensors\n",
        "* `q4_0`: Original quant method, 4-bit.\n",
        "* `q4_1`: Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.\n",
        "* `q4_k_m`: Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K\n",
        "* `q4_k_s`: Uses Q4_K for all tensors\n",
        "* `q5_0`: Higher accuracy, higher resource usage and slower inference.\n",
        "* `q5_1`: Even higher accuracy, resource usage and slower inference.\n",
        "* `q5_k_m`: Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K\n",
        "* `q5_k_s`:  Uses Q5_K for all tensors\n",
        "* `q6_k`: Uses Q8_K for all tensors\n",
        "* `q8_0`: Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.\n",
        "\n",
        "As a rule of thumb, **I recommend using Q5_K_M** as it preserves most of the model's performance. Alternatively, you can use Q4_K_M if you want to save some memory. In general, K_M versions are better than K_S versions. I cannot recommend Q2_K or Q3_* versions, as they drastically decrease model performance."
      ],
      "metadata": {
        "id": "Pkp9F0KxtDJN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1 - Clone the llama.cpp Git Repository"
      ],
      "metadata": {
        "id": "1ahDltWGtDMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp"
      ],
      "metadata": {
        "id": "PAU99Gn-tdwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2 - Install the Requirements"
      ],
      "metadata": {
        "id": "h5EHqVebtDPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && LLAMA_CUBLAS=1 make && pip install -r requirements/requirements-convert-hf-to-gguf.txt"
      ],
      "metadata": {
        "id": "pN0ZmXBsttN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 3 - Initialize the Model Name and its Method to Quantize"
      ],
      "metadata": {
        "id": "C775W7uhtomo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "model_name = \"\"  # Change the model name as your wish. For eg -\"microsoft/phi-1_5\"\n",
        "quantization_methods = ['']   # Change the model quantization methods type as your wish. For eg - ['q5_k_m']\n",
        "hf_user_name = \"\"   # Change the HF User Name. For eg - \"santhoshmlops\"\n",
        "base_model = \"./original_model/\"\n",
        "quantized_path = \"./quantized_model/\"\n",
        "qtype = f\"{quantized_path}{quantization_methods[0].upper()}.gguf\"\n",
        "original_model = quantized_path+'/FP16.gguf'\n",
        "snapshot_download(repo_id=model_name, local_dir=base_model , local_dir_use_symlinks=False)"
      ],
      "metadata": {
        "id": "dlmuKVdbti2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 4 - Make a Directory for Quantized model"
      ],
      "metadata": {
        "id": "vE0Xxlp5t1F6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ./quantized_model/\n",
        "!python llama.cpp/convert-hf-to-gguf.py ./original_model/ --outtype f16 --outfile ./quantized_model/FP16.gguf"
      ],
      "metadata": {
        "id": "2D3MC3WVt3Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5 - Build the Quantized Model\n",
        "\n",
        "`Note:` You can stop the run cell once you're okay with the user interacting with Bob the Assistant"
      ],
      "metadata": {
        "id": "34e-Lqb3t6dj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for m in quantization_methods:\n",
        "    qtype = f\"{quantized_path}/{m.upper()}.gguf\"\n",
        "    os.system(\"./llama.cpp/quantize \"+quantized_path+\"/FP16.gguf \"+qtype+\" \"+m)\n",
        "\n",
        "! ./llama.cpp/main -m {qtype} -n 90 --repeat_penalty 1.0 --color -i -r \"User:\" -f llama.cpp/prompts/chat-with-bob.txt"
      ],
      "metadata": {
        "id": "qWOVWhY6t8eU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6 - Login to your Hugging Face Hub\n",
        "`Note:`  If you have already set the HF_TOKEN secret key, you can skip this step"
      ],
      "metadata": {
        "id": "BPEukmLiuA1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "46zNn7PLuCgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7 -  Initialize the Model path and Repository name"
      ],
      "metadata": {
        "id": "2lXUozRHuEnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, HfFolder, create_repo, upload_file\n",
        "model_path = qtype\n",
        "user_name = hf_user_name\n",
        "repo_name = model_name.split(\"/\")[-1]+\"-GGUF\"\n",
        "repo_path = model_name.split(\"/\")[-1].lower()+\".\"+model_path.split(\"/\")[-1]\n",
        "repo_id = user_name+\"/\"+repo_name\n",
        "repo_type = \"model\"\n",
        "repo_url = create_repo(repo_name, private=False)"
      ],
      "metadata": {
        "id": "oh9gmYeJuGDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8 - Push the Quantized model to Hub"
      ],
      "metadata": {
        "id": "Dx8pqTk_uIoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api = HfApi()\n",
        "api.upload_file(\n",
        "    path_or_fileobj = model_path,\n",
        "    path_in_repo = repo_path,\n",
        "    repo_id = repo_id,\n",
        "    repo_type = repo_type,\n",
        ")"
      ],
      "metadata": {
        "id": "J7AecbREuLXC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}