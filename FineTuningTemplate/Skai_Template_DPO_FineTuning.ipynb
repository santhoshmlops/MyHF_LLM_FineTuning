{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Direct Preference Optimization (DPO) Fine-Tuning Template"
      ],
      "metadata": {
        "id": "TQJhp1_exyZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "Supervised fine-tuning (SFT) is the first step of (DPO).During the fine-tuning phase, DPO uses the LLM as a reward model. It optimizes the policy using a binary cross-entropy objective, leveraging human preference data to determine which responses are preferred and which are not. By comparing the model's responses to the preferred ones, the policy is adjusted to enhance its performance.\n",
        "\n",
        "`I prepared this Direct Preference Optimization (DPO) template for my use case, but you could change it to suit your requirements.`\n",
        "\n",
        "\n",
        "\n",
        "To View My Account:\n",
        "\n",
        "* [Hugging Face ](https://huggingface.co/santhoshmlops)\n",
        "\n",
        "* [Git Hub](https://github.com/santhoshmlops)\n",
        "\n",
        "To View Some other Fine Tuning Template:\n",
        "\n",
        "* [Fine Tuning Template ](https://github.com/santhoshmlops/MyHF_LLM_FineTuning/tree/main/FineTuningTemplate)\n",
        "\n",
        "\n",
        "To View My Model Fine Tuning  NoteBook:\n",
        "\n",
        "* [MY HF LLM Fine-Tuning](https://github.com/santhoshmlops/MyHF_LLM_FineTuning)\n",
        "\n"
      ],
      "metadata": {
        "id": "EccxiIzQyHje"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up on Google Colab\n",
        "Google Colab provides a convenient, cloud-based environment with access to powerful GPUs like the `T4`. If you choose Colab for this tutorial, make sure to select a GPU runtime by going to `Runtime > Change runtime type > T4 GPU`. This ensures that your notebook has access to the necessary computational resources."
      ],
      "metadata": {
        "id": "Rj8jNEOWyTZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up Hugging Face Authentication\n",
        "\n",
        "On Google Colab, you can safely store your Hugging Face token by using Colab's \"Secrets\" feature. This can be done by clicking on the \"Key\" icon in the sidebar, selecting \"`Secrets`\", and adding a new secret with the name `HF_TOKEN` and your Hugging Face token as the value. This method ensures that your token remains secure and is not exposed in your notebook's code."
      ],
      "metadata": {
        "id": "w9CQq_XDyYAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1 - Install the required Python packages"
      ],
      "metadata": {
        "id": "a-Ei9GrHyyZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U transformers\n",
        "!pip install -q -U peft\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U trl\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U datasets"
      ],
      "metadata": {
        "id": "Loxa-902yU6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2 - Logging into Hugging Face Hub\n",
        "Paste the Hugging Face Hub Write API KEY"
      ],
      "metadata": {
        "id": "ugBIiGYfy3F6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLZkfGTFxrbr"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3 - Loading Required Libraries"
      ],
      "metadata": {
        "id": "x2ZZvg0My8h6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
        "from peft import get_peft_model, LoraConfig,prepare_model_for_kbit_training\n",
        "from trl import DPOTrainer\n",
        "from accelerate import Accelerator"
      ],
      "metadata": {
        "id": "5HcFxwQFy-oV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4 - Setting Model Parameters for DPO\n",
        "`Note:` The parameter can be changed for fine tuning, or it can be left as it is and filled with the value of the empty parameter."
      ],
      "metadata": {
        "id": "VOdfahrNzBfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dpo_config = {\n",
        "            # Load Model for Tuning\n",
        "            \"model_ckpt\": \"\",  # Use SFT Trained model to perform DPO Fine Tuning\n",
        "            \"load_in_4bit\": True,\n",
        "            \"device_map\": {\"\": Accelerator().local_process_index},\n",
        "            \"torch_dtype\": torch.float16,\n",
        "            \"trust_remote_code\": True,\n",
        "            # QLora Parameters\n",
        "            \"use_lora\": True,\n",
        "            \"r\": 8,\n",
        "            \"lora_alpha\": 8,\n",
        "            \"lora_dropout\": 0.05,\n",
        "            \"bias\": \"none\",\n",
        "            \"task_type\": \"CAUSAL_LM\",\n",
        "            \"target_modules\": [\"q_proj\",\"k_proj\"],\n",
        "            # Training Parameters\n",
        "            \"output_dir\": \"Fine_tuned_model-DPO\",\n",
        "            \"per_device_train_batch_size\": 5,\n",
        "            \"gradient_accumulation_steps\": 1,\n",
        "            \"optim\": \"paged_adamw_32bit\",\n",
        "            \"learning_rate\": 2e-4,\n",
        "            \"lr_scheduler_type\": \"cosine\",\n",
        "            \"save_strategy\": \"epoch\",\n",
        "            \"logging_steps\": 50,\n",
        "            \"num_train_epochs\": 1,\n",
        "            \"max_steps\": 500,\n",
        "            \"fp16\": True,\n",
        "            \"push_to_hub\": True,\n",
        "            \"train_cln_name\": \"text\",\n",
        "            \"packing\": False,\n",
        "            \"neftune_noise_alpha\": 5,\n",
        "            # SFT Training Parameter\n",
        "            \"beta\": 0.1,\n",
        "            \"loss_type\": \"kto_pair\",\n",
        "            \"max_length\": 768,\n",
        "            \"max_prompt_length\": 512,\n",
        "            \"max_target_length\": 256,\n",
        "            \"is_encoder_decoder\": False\n",
        "        }"
      ],
      "metadata": {
        "id": "3jwmkLgszEDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5 - Loading and Formatting the Dataset\n",
        "`Note:` Prepare your dataset for fine tuning by defining and formatting it for your use case. The `def create_data():` function is an example for tuning the dataset."
      ],
      "metadata": {
        "id": "2UAGVm3QzGdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"Anthropic/hh-rlhf\"\n",
        "def create_data():\n",
        "  data = load_dataset(dataset_name, split=\"train\")\n",
        "  data_df = data.to_pandas()\n",
        "  data_df[\"prompt\"] = data_df[\"chosen\"].apply(lambda x: x.split(\"Assistant: \")[0])\n",
        "  data_df[\"chosen\"] = data_df[\"chosen\"].apply(lambda x: \"Assistant: \"+ x.split(\"Assistant: \")[-1])\n",
        "  data_df[\"rejected\"] = data_df[\"rejected\"].apply(lambda x: \"Assistant: \" + x.split(\"Assistant: \")[-1])\n",
        "  data = Dataset.from_pandas(data_df)\n",
        "  return data\n",
        "\n",
        "data = create_data()\n",
        "print(data[0])"
      ],
      "metadata": {
        "id": "9UsHfAbizInl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6 - Fine-Tuning with qLora and Supervised Finetuning"
      ],
      "metadata": {
        "id": "0We6QTCPzK_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainDPO:\n",
        "\n",
        "    def __init__(self, data, config):\n",
        "        \"\"\"\n",
        "        Initialize the TrainSFT class with data and configuration parameters.\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.config = config\n",
        "\n",
        "    def prepare_lora_model(self):\n",
        "        \"\"\"\n",
        "        Prepare the model with LoRA (Low-Rank Adaptation) configuration.\n",
        "        \"\"\"\n",
        "\n",
        "        self.lora_config = LoraConfig(\n",
        "                                    r=self.config[\"r\"],\n",
        "                                    lora_alpha=self.config[\"lora_alpha\"],\n",
        "                                    lora_dropout=self.config[\"lora_dropout\"],\n",
        "                                    bias=self.config[\"bias\"],\n",
        "                                    task_type=self.config[\"task_type\"],\n",
        "                                    target_modules=self.config[\"target_modules\"]\n",
        "                                )\n",
        "        self.model = get_peft_model(self.model, self.lora_config)\n",
        "        self.model_ref = get_peft_model(self.model_ref, self.lora_config)\n",
        "\n",
        "    def load_model_tokenizer(self):\n",
        "        \"\"\"\n",
        "        Load the model and tokenizer with specified configurations.\n",
        "        \"\"\"\n",
        "\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                            self.config[\"model_ckpt\"],\n",
        "                            load_in_4bit=self.config[\"load_in_4bit\"],\n",
        "                            device_map=self.config[\"device_map\"],\n",
        "                            torch_dtype=self.config[\"torch_dtype\"]\n",
        "                        )\n",
        "\n",
        "        self.model_ref = AutoModelForCausalLM.from_pretrained(\n",
        "                            self.config[\"model_ckpt\"],\n",
        "                            load_in_4bit=self.config[\"load_in_4bit\"],\n",
        "                            device_map=self.config[\"device_map\"],\n",
        "                            torch_dtype=self.config[\"torch_dtype\"]\n",
        "                        )\n",
        "        self.model.config.use_cache=False\n",
        "        self.model.config.pretraining_tp=1\n",
        "        self.model = prepare_model_for_kbit_training(self.model)\n",
        "        if self.config[\"use_lora\"]:\n",
        "            self.prepare_lora_model()\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.config[\"model_ckpt\"])\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "    def set_training_args(self):\n",
        "        \"\"\"\n",
        "        Set up training arguments.\n",
        "        \"\"\"\n",
        "\n",
        "        return TrainingArguments(\n",
        "                                    output_dir=self.config[\"output_dir\"],\n",
        "                                    per_device_train_batch_size=self.config[\"per_device_train_batch_size\"],\n",
        "                                    gradient_accumulation_steps=self.config[\"gradient_accumulation_steps\"],\n",
        "                                    optim=self.config[\"optim\"],\n",
        "                                    learning_rate=self.config[\"learning_rate\"],\n",
        "                                    lr_scheduler_type=self.config[\"lr_scheduler_type\"],\n",
        "                                    save_strategy=self.config[\"save_strategy\"],\n",
        "                                    logging_steps=self.config[\"logging_steps\"],\n",
        "                                    num_train_epochs=self.config[\"num_train_epochs\"],\n",
        "                                    max_steps=self.config[\"max_steps\"],\n",
        "                                    fp16=self.config[\"fp16\"],\n",
        "                                    push_to_hub=self.config[\"push_to_hub\"],\n",
        "                                    neftune_noise_alpha=self.config[\"neftune_noise_alpha\"]\n",
        "                                )\n",
        "\n",
        "    def create_trainer(self):\n",
        "        \"\"\"\n",
        "        Create a trainer for training the model.\n",
        "        \"\"\"\n",
        "\n",
        "        self.load_model_tokenizer()\n",
        "        if self.config[\"use_lora\"]:\n",
        "            print(self.model.print_trainable_parameters())\n",
        "            self.trainer = DPOTrainer(\n",
        "                                        self.model,\n",
        "                                        self.model_ref,\n",
        "                                        args=self.set_training_args(),\n",
        "                                        train_dataset=self.data,\n",
        "                                        tokenizer=self.tokenizer,\n",
        "                                        beta=self.config[\"beta\"],\n",
        "                                        loss_type=self.config[\"loss_type\"],\n",
        "                                        max_length=self.config[\"max_length\"],\n",
        "                                        max_prompt_length=self.config[\"max_prompt_length\"],\n",
        "                                        max_target_length=self.config[\"max_target_length\"],\n",
        "                                        is_encoder_decoder=self.config[\"is_encoder_decoder\"]\n",
        "                                    )\n",
        "        else:\n",
        "            self.trainer = DPOTrainer(\n",
        "                                        self.model,\n",
        "                                        self.model_ref,\n",
        "                                        peft_config=self.lora_config,\n",
        "                                        args=self.set_training_args(),\n",
        "                                        train_dataset=self.data,\n",
        "                                        tokenizer=self.tokenizer,\n",
        "                                        beta=self.config[\"beta\"],\n",
        "                                        loss_type=self.config[\"loss_type\"],\n",
        "                                        max_length=self.config[\"max_length\"],\n",
        "                                        max_prompt_length=self.config[\"max_prompt_length\"],\n",
        "                                        max_target_length=self.config[\"max_target_length\"],\n",
        "                                        is_encoder_decoder=self.config[\"is_encoder_decoder\"]\n",
        "                                    )\n",
        "\n",
        "    def train_and_save_model(self):\n",
        "        \"\"\"\n",
        "        Train the model and save it.\n",
        "        \"\"\"\n",
        "        self.create_trainer()\n",
        "        self.trainer.train()\n",
        "        self.trainer.save_model(self.config[\"output_dir\"])\n",
        "        self.tokenizer.save_pretrained(self.config[\"output_dir\"])\n"
      ],
      "metadata": {
        "id": "0xQoDbB4zNMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7 - Lets start the training process"
      ],
      "metadata": {
        "id": "2N9VTOBGzPTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dpo = TrainDPO(data, dpo_config)\n",
        "train_dpo.train_and_save_model()"
      ],
      "metadata": {
        "id": "jlaFWJ90zQ5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8 - Merge the model with LoRA weights"
      ],
      "metadata": {
        "id": "TIFy-Q4mzTDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_push_to_hub(config):\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config[\"model_ckpt\"])\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "\n",
        "    # Load base model\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        config[\"model_ckpt\"],\n",
        "        low_cpu_mem_usage=config[\"low_cpu_mem_usage\"],\n",
        "        return_dict=config[\"return_dict\"],\n",
        "        torch_dtype=config[\"torch_dtype\"],\n",
        "        device_map=config[\"device_map\"]\n",
        "    )\n",
        "\n",
        "    # Merge models\n",
        "    merged_model = PeftModel.from_pretrained(base_model,config[\"hub_model_ckpt\"], from_transformers=True)\n",
        "    merged_model = merged_model.merge_and_unload()\n",
        "\n",
        "    # Push the model and tokenizer to the Hugging Face Model Hub\n",
        "    merged_model.push_to_hub(config[\"new_model_ckpt\"], use_temp_dir=False)\n",
        "    tokenizer.push_to_hub(config[\"new_model_ckpt\"], use_temp_dir=False)\n",
        "\n",
        "# Assuming sft_config is defined elsewhere\n",
        "merge_push_to_hub(sft_config)"
      ],
      "metadata": {
        "id": "yOSPVFTRzVYs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}