{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQJhp1_exyZm"
      },
      "source": [
        "# Supervised Fine-Tuning (SFT) Simple Template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EccxiIzQyHje"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "Supervised fine-tuning (SFT) is a technique used to adapt a pre-trained Large Language Model (LLM) to a specific downstream task using labeled data.This process allows the model to learn task-specific patterns and nuances by adapting its parameters according to the specific data distribution and task requirements.\n",
        "\n",
        "`I prepared this Supervised Fine-Tuning (SFT) template for my use case, but you could change it to suit your requirements.`\n",
        "\n",
        "\n",
        "\n",
        "To View My Account:\n",
        "\n",
        "* [Hugging Face ](https://huggingface.co/santhoshmlops)\n",
        "\n",
        "* [Git Hub](https://github.com/santhoshmlops)\n",
        "\n",
        "To View Some other Fine Tuning Template:\n",
        "\n",
        "* [Fine Tuning Template ](https://github.com/santhoshmlops/MyHF_LLM_FineTuning/tree/main/FineTuningTemplate)\n",
        "\n",
        "\n",
        "To View My Model Fine Tuning  NoteBook:\n",
        "\n",
        "* [MY HF LLM Fine-Tuning](https://github.com/santhoshmlops/MyHF_LLM_FineTuning)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj8jNEOWyTZG"
      },
      "source": [
        "## Setting Up on Google Colab\n",
        "Google Colab provides a convenient, cloud-based environment with access to powerful GPUs like the `T4`. If you choose Colab for this tutorial, make sure to select a GPU runtime by going to `Runtime > Change runtime type > T4 GPU`. This ensures that your notebook has access to the necessary computational resources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9CQq_XDyYAO"
      },
      "source": [
        "## Setting Up Hugging Face Authentication\n",
        "\n",
        "On Google Colab, you can safely store your Hugging Face token by using Colab's \"Secrets\" feature. This can be done by clicking on the \"Key\" icon in the sidebar, selecting \"`Secrets`\", and adding a new secret with the name `HF_TOKEN` and your Hugging Face token as the value. This method ensures that your token remains secure and is not exposed in your notebook's code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-Ei9GrHyyZL"
      },
      "source": [
        "# Step 1 - Install the required Python packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Loxa-902yU6W"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U transformers\n",
        "!pip install -q -U peft\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U trl\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugBIiGYfy3F6"
      },
      "source": [
        "# Step 2 - Logging into Hugging Face Hub\n",
        "Paste the Hugging Face Hub Write API KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLZkfGTFxrbr"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2ZZvg0My8h6"
      },
      "source": [
        "# Step 3 - Loading Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HcFxwQFy-oV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments,DataCollatorForLanguageModeling\n",
        "from peft import LoraConfig,PeftModel, AutoPeftModelForCausalLM, prepare_model_for_kbit_training, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "from accelerate import Accelerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOdfahrNzBfb"
      },
      "source": [
        "# Step 4 - Setting Model Parameters for SFT\n",
        "`Note:` The parameter can be changed for fine tuning, or it can be left as it is and filled with the value of the empty parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jwmkLgszEDt"
      },
      "outputs": [],
      "source": [
        "# Load Model for Tuning\n",
        "model_ckpt = \"\"  # Change the model_ckpt as your wish. For eg - \"microsoft/phi-1_5\"\n",
        "hf_user_name = \"santhoshmlops\"\n",
        "hub_model_ckpt = hf_user_name+\"/\"+ model_ckpt.split(\"/\")[-1]+\"-SFT\" # Change the hub_model_ckpt as your wish. For eg - \"santhoshmlops/microsoft_phi-1_5_merged-SFT\"\n",
        "dataset_name = \"\"\n",
        "\n",
        "# Lora Parameters\n",
        "r= 16\n",
        "lora_alpha = 32\n",
        "lora_dropout = 0.05\n",
        "bias = \"none\"\n",
        "task_type = \"CAUSAL_LM\"\n",
        "target_modules = [\"q_proj\",\"k_proj\", \"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]    # Change the Target modules based on the model for tuning For eg - [\"q_proj\",\"k_proj\"]\n",
        "\n",
        "# BitsandBytes Parameters\n",
        "load_in_4bit = True\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "bnb_4bit_compute_dtype = torch.float16\n",
        "bnb_4bit_use_double_quant = True\n",
        "\n",
        "# Automodel Parameters\n",
        "device_map = {\"\": Accelerator().local_process_index}\n",
        "torch_dtype = torch.float16\n",
        "\n",
        "# Tokenizer Parameters\n",
        "trust_remote_code = True\n",
        "\n",
        "# Training Parameters\n",
        "output_dir = model_ckpt.split(\"/\")[-1]+\"-SFT\"   # Change the model_ckpt as your wish. For eg - \"microsoft_phi-1_5_merged-SFT\"\n",
        "num_train_epochs = 1\n",
        "per_device_train_batch_size = 2\n",
        "gradient_accumulation_steps = 2\n",
        "gradient_checkpointing = True\n",
        "max_grad_norm = 0.3\n",
        "learning_rate = 2e-4\n",
        "weight_decay = 0.003\n",
        "optim = \"paged_adamw_8bit\"\n",
        "lr_scheduler_type = \"cosine\"\n",
        "max_steps = 750\n",
        "warmup_ratio = 0.03\n",
        "group_by_length = True\n",
        "save_steps = 100\n",
        "save_strategy = \"epoch\"\n",
        "logging_steps = 100\n",
        "logging_dir = \"./logs\"\n",
        "fp16 = True\n",
        "bf16 = False\n",
        "push_to_hub = True\n",
        "neftune_noise_alpha = 5\n",
        "report_to = \"tensorboard\"\n",
        "\n",
        "# SFT Training Parameters\n",
        "train_cln_name = \"text\"\n",
        "packing = False\n",
        "max_seq_length = 1024\n",
        "\n",
        "# Merge and push the model to Hub\n",
        "low_cpu_mem_usage = True\n",
        "return_dict = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UAGVm3QzGdq"
      },
      "source": [
        "# Step 5 - Loading and Formatting the Dataset\n",
        "`Note:` Prepare your dataset for fine tuning by defining and formatting it for your use case. The `def create_data():` function is an example for tuning the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UsHfAbizInl"
      },
      "outputs": [],
      "source": [
        "def create_data():\n",
        "  data = load_dataset(dataset_name, split=\"train\")\n",
        "  return data\n",
        "\n",
        "data = create_data()\n",
        "print(data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0We6QTCPzK_y"
      },
      "source": [
        "# Step 6 - Fine-Tuning with Lora and Supervised Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ng1J_cShXLY3"
      },
      "outputs": [],
      "source": [
        "# Load the model and tokenizer with specified configurations.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_ckpt,\n",
        "    trust_remote_code=trust_remote_code\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=load_in_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,\n",
        "    bnb_4bit_use_double_quant=bnb_4bit_use_double_quant\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_ckpt,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map,\n",
        "    trust_remote_code=trust_remote_code,\n",
        "    torch_dtype=torch_dtype\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    gradient_checkpointing=gradient_checkpointing,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    optim=optim,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    save_steps=save_steps,\n",
        "    save_strategy=save_strategy,\n",
        "    logging_steps=logging_steps,\n",
        "    logging_dir=logging_dir,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    push_to_hub=push_to_hub,\n",
        "    neftune_noise_alpha = neftune_noise_alpha,\n",
        "    report_to=report_to\n",
        ")\n",
        "\n",
        "# Prepare the model with LoRA (Low-Rank Adaptation) configuration.\n",
        "lora_config = LoraConfig(\n",
        "    r=r,\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    bias=bias,\n",
        "    task_type=task_type,\n",
        "    target_modules=target_modules\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Create a trainer for training the model.\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=data,\n",
        "    peft_config=lora_config,\n",
        "    dataset_text_field=train_cln_name,\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer,\n",
        "    packing=packing,\n",
        "    max_seq_length=max_seq_length,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2N9VTOBGzPTC"
      },
      "source": [
        "# Step 7 - Lets start the training process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlaFWJ90zQ5M"
      },
      "outputs": [],
      "source": [
        "# Train the model and save it.\n",
        "trainer.train()\n",
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIFy-Q4mzTDq"
      },
      "source": [
        "# Step 8 - Merge the model with LoRA weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOSPVFTRzVYs"
      },
      "outputs": [],
      "source": [
        "# Clear the memory footprint\n",
        "del model, trainer\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt,\n",
        "                                          trust_remote_code=trust_remote_code)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Load base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_ckpt,\n",
        "                                                  low_cpu_mem_usage=low_cpu_mem_usage,\n",
        "                                                  return_dict=return_dict,\n",
        "                                                  torch_dtype=torch_dtype,\n",
        "                                                  device_map=device_map,trust_remote_code=trust_remote_code)\n",
        "\n",
        "# Merge models\n",
        "merged_model = PeftModel.from_pretrained(base_model,hub_model_ckpt, from_transformers=True)\n",
        "merged_model = merged_model.merge_and_unload()\n",
        "\n",
        "# Push the model and tokenizer to the Hugging Face Model Hub\n",
        "merged_model.push_to_hub(hub_model_ckpt, use_temp_dir=False)\n",
        "tokenizer.push_to_hub(hub_model_ckpt, use_temp_dir=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8 - Inferencing with the model output"
      ],
      "metadata": {
        "id": "2Z8XjI8sLKvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install bitsandbytes accelerate\n",
        "import time\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"santhoshmlops/gemma-2b-it-SFT\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"santhoshmlops/gemma-2b-it-SFT\", quantization_config=quantization_config)\n",
        "\n",
        "st_time = time.time()\n",
        "\n",
        "input_text = \"Write me a poem about Machine Learning.\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**input_ids,max_new_tokens=100)\n",
        "print(tokenizer.decode(outputs[0]))\n",
        "\n",
        "print(time.time()-st_time)"
      ],
      "metadata": {
        "id": "fxRFrZeWHzNR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}