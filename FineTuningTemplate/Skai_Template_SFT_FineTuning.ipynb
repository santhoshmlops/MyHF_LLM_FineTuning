{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised Fine-Tuning (SFT) Template"
      ],
      "metadata": {
        "id": "TQJhp1_exyZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "Supervised fine-tuning (SFT) is a technique used to adapt a pre-trained Large Language Model (LLM) to a specific downstream task using labeled data.This process allows the model to learn task-specific patterns and nuances by adapting its parameters according to the specific data distribution and task requirements.\n",
        "\n",
        "`I prepared this Supervised Fine-Tuning (SFT) template for my use case, but you could change it to suit your requirements.`\n",
        "\n",
        "\n",
        "\n",
        "To View My Account:\n",
        "\n",
        "* [Hugging Face ](https://huggingface.co/santhoshmlops)\n",
        "\n",
        "* [Git Hub](https://github.com/santhoshmlops)\n",
        "\n",
        "To View Some other Fine Tuning Template:\n",
        "\n",
        "* [Fine Tuning Template ](https://github.com/santhoshmlops/MyHF_LLM_FineTuning/tree/main/FineTuningTemplate)\n",
        "\n",
        "\n",
        "To View My Model Fine Tuning  NoteBook:\n",
        "\n",
        "* [MY HF LLM Fine-Tuning](https://github.com/santhoshmlops/MyHF_LLM_FineTuning)\n",
        "\n"
      ],
      "metadata": {
        "id": "EccxiIzQyHje"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up on Google Colab\n",
        "Google Colab provides a convenient, cloud-based environment with access to powerful GPUs like the `T4`. If you choose Colab for this tutorial, make sure to select a GPU runtime by going to `Runtime > Change runtime type > T4 GPU`. This ensures that your notebook has access to the necessary computational resources."
      ],
      "metadata": {
        "id": "Rj8jNEOWyTZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up Hugging Face Authentication\n",
        "\n",
        "On Google Colab, you can safely store your Hugging Face token by using Colab's \"Secrets\" feature. This can be done by clicking on the \"Key\" icon in the sidebar, selecting \"`Secrets`\", and adding a new secret with the name `HF_TOKEN` and your Hugging Face token as the value. This method ensures that your token remains secure and is not exposed in your notebook's code."
      ],
      "metadata": {
        "id": "w9CQq_XDyYAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1 - Install the required Python packages"
      ],
      "metadata": {
        "id": "a-Ei9GrHyyZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U transformers\n",
        "!pip install -q -U peft\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U trl\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U datasets"
      ],
      "metadata": {
        "id": "Loxa-902yU6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2 - Logging into Hugging Face Hub\n",
        "Paste the Hugging Face Hub Write API KEY"
      ],
      "metadata": {
        "id": "ugBIiGYfy3F6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLZkfGTFxrbr"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3 - Loading Required Libraries"
      ],
      "metadata": {
        "id": "x2ZZvg0My8h6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
        "from peft import LoraConfig,PeftModel, AutoPeftModelForCausalLM, prepare_model_for_kbit_training, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "from accelerate import Accelerator"
      ],
      "metadata": {
        "id": "5HcFxwQFy-oV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4 - Setting Model Parameters for SFT\n",
        "`Note:` The parameter can be changed for fine tuning, or it can be left as it is and filled with the value of the empty parameter."
      ],
      "metadata": {
        "id": "VOdfahrNzBfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sft_config = {\n",
        "            # Load Model for Tuning\n",
        "            \"model_ckpt\": \"\",  # Change the model_ckpt as your wish. For eg - \"microsoft/phi-1_5\"\n",
        "            \"new_model_ckpt\": \"\", # Change the new_model_ckpt as your wish. For eg - \"microsoft_phi-1_5_merged-SFT\"\n",
        "            \"hub_model_ckpt\": \"\", # Change the hub_model_ckpt as your wish. For eg - \"santhoshmlops/microsoft_phi-1_5_merged-SFT\"\n",
        "            # QLora Parameters\n",
        "            \"use_lora\": True,\n",
        "            \"r\": 16,\n",
        "            \"lora_alpha\": 32,\n",
        "            \"lora_dropout\": 0.05,\n",
        "            \"bias\": \"none\",\n",
        "            \"task_type\": \"CAUSAL_LM\",\n",
        "            \"target_modules\": [\"\"],    # Change the Target modules based on the model for tuning For eg - [\"q_proj\",\"k_proj\"]\n",
        "            # BitsandBytes Parameters\n",
        "            \"load_in_4bit\": True,\n",
        "            \"bnb_4bit_quant_type\" : \"nf4\",\n",
        "            \"bnb_4bit_compute_dtype\": torch.float16,\n",
        "            \"bnb_4bit_use_double_quant\": True,\n",
        "            # Automodel Parameters\n",
        "            \"device_map\": {\"\": Accelerator().local_process_index},\n",
        "            \"torch_dtype\": torch.float16,\n",
        "            # Tokenizer Parameters\n",
        "            \"trust_remote_code\": True,\n",
        "            # Training Parameters\n",
        "            \"output_dir\": \"\",   # Change the model_ckpt as your wish. For eg - \"microsoft_phi-1_5_merged-SFT\"\n",
        "            \"num_train_epochs\": 1,\n",
        "            \"per_device_train_batch_size\": 1,\n",
        "            \"gradient_accumulation_steps\": 1,\n",
        "            \"gradient_checkpointing\" : True,\n",
        "            \"max_grad_norm\" : 0.3,\n",
        "            \"learning_rate\": 2e-4,\n",
        "            \"weight_decay\" : 0.003,\n",
        "            \"optim\": \"paged_adamw_32bit\",\n",
        "            \"lr_scheduler_type\": \"cosine\",\n",
        "            \"max_steps\": 10,\n",
        "            \"warmup_ratio\" : 0.03,\n",
        "            \"group_by_length\" : True,\n",
        "            \"save_steps\" : 2,\n",
        "            \"save_strategy\": \"epoch\",\n",
        "            \"logging_steps\": 2,\n",
        "            \"logging_dir\": \"./logs\",\n",
        "            \"fp16\": False,\n",
        "            \"bf16\" : False,\n",
        "            \"push_to_hub\": True,\n",
        "            \"neftune_noise_alpha\": 5,\n",
        "            \"report_to\":\"tensorboard\",\n",
        "            # SFT Training Parameters\n",
        "            \"train_cln_name\": \"chat_sample\",\n",
        "            \"packing\": False,\n",
        "            \"max_seq_length\": 512,\n",
        "            # Merge and push the model to Hub\n",
        "            \"low_cpu_mem_usage\" : True,\n",
        "            \"return_dict\" : True,\n",
        "            \"torch_dtype\": torch.float16\n",
        "        }"
      ],
      "metadata": {
        "id": "3jwmkLgszEDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5 - Loading and Formatting the Dataset\n",
        "`Note:` Prepare your dataset for fine tuning by defining and formatting it for your use case. The `def create_data():` function is an example for tuning the dataset."
      ],
      "metadata": {
        "id": "2UAGVm3QzGdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"gathnex/Gath_baize\"\n",
        "def create_data():\n",
        "  data = load_dataset(dataset_name, split=\"train\")\n",
        "  data_df = data.to_pandas()\n",
        "  original_system_message = \"The conversation between Human and AI assisatance named Gathnex\"\n",
        "  system_message = \"[INST]The conversation between Human and AI assisatance named Microsoft_Phi AI Assisatance.\\n[/INST]\"\n",
        "  data_df[\"chat_sample\"] = data_df[\"chat_sample\"].apply(lambda x: x.replace(original_system_message, \"\").strip())\n",
        "  data_df[\"chat_sample\"]= system_message + data_df[\"chat_sample\"]\n",
        "  data = Dataset.from_pandas(data_df)\n",
        "  return data\n",
        "\n",
        "data = create_data()\n",
        "print(data[0])"
      ],
      "metadata": {
        "id": "9UsHfAbizInl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6 - Fine-Tuning with qLora and Supervised Finetuning"
      ],
      "metadata": {
        "id": "0We6QTCPzK_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Class definition for training SFT (Self-Fine-Tuning) models\n",
        "class TrainSFT:\n",
        "\n",
        "    def __init__(self, data, config):\n",
        "        \"\"\"\n",
        "        Initialize the TrainSFT class with data and configuration parameters.\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.config = config\n",
        "\n",
        "    def prepare_lora_model(self):\n",
        "        \"\"\"\n",
        "        Prepare the model with LoRA (Low-Rank Adaptation) configuration.\n",
        "        \"\"\"\n",
        "        self.lora_config = LoraConfig(\n",
        "            r=self.config[\"r\"],\n",
        "            lora_alpha=self.config[\"lora_alpha\"],\n",
        "            lora_dropout=self.config[\"lora_dropout\"],\n",
        "            bias=self.config[\"bias\"],\n",
        "            task_type=self.config[\"task_type\"],\n",
        "            target_modules=self.config[\"target_modules\"]\n",
        "        )\n",
        "        self.model = get_peft_model(self.model, self.lora_config)\n",
        "\n",
        "    def load_model_tokenizer(self):\n",
        "        \"\"\"\n",
        "        Load the model and tokenizer with specified configurations.\n",
        "        \"\"\"\n",
        "        self.bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=self.config[\"load_in_4bit\"],\n",
        "            bnb_4bit_quant_type=self.config[\"bnb_4bit_quant_type\"],\n",
        "            bnb_4bit_compute_dtype=self.config[\"bnb_4bit_compute_dtype\"],\n",
        "            bnb_4bit_use_double_quant=self.config[\"bnb_4bit_use_double_quant\"],\n",
        "        )\n",
        "\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.config[\"model_ckpt\"],\n",
        "            quantization_config=self.bnb_config,\n",
        "            device_map=self.config[\"device_map\"],\n",
        "            trust_remote_code=self.config[\"trust_remote_code\"],\n",
        "            torch_dtype=self.config[\"torch_dtype\"]\n",
        "        )\n",
        "        self.model.config.use_cache = False\n",
        "        self.model.config.pretraining_tp = 1\n",
        "        self.model.gradient_checkpointing_enable()\n",
        "        self.model = prepare_model_for_kbit_training(self.model)\n",
        "\n",
        "        if self.config[\"use_lora\"]:\n",
        "            self.prepare_lora_model()\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            self.config[\"model_ckpt\"],\n",
        "            trust_remote_code=self.config[\"trust_remote_code\"],\n",
        "            )\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.tokenizer.padding_side = \"right\"\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    def set_training_args(self):\n",
        "        \"\"\"\n",
        "        Set up training arguments.\n",
        "        \"\"\"\n",
        "        return TrainingArguments(\n",
        "            output_dir=self.config[\"output_dir\"],\n",
        "            num_train_epochs=self.config[\"num_train_epochs\"],\n",
        "            per_device_train_batch_size=self.config[\"per_device_train_batch_size\"],\n",
        "            gradient_accumulation_steps=self.config[\"gradient_accumulation_steps\"],\n",
        "            gradient_checkpointing=self.config[\"gradient_checkpointing\"],\n",
        "            max_grad_norm=self.config[\"max_grad_norm\"],\n",
        "            learning_rate=self.config[\"learning_rate\"],\n",
        "            weight_decay=self.config[\"weight_decay\"],\n",
        "            optim=self.config[\"optim\"],\n",
        "            lr_scheduler_type=self.config[\"lr_scheduler_type\"],\n",
        "            max_steps=self.config[\"max_steps\"],\n",
        "            warmup_ratio=self.config[\"warmup_ratio\"],\n",
        "            group_by_length=self.config[\"group_by_length\"],\n",
        "            save_steps=self.config[\"save_steps\"],\n",
        "            save_strategy=self.config[\"save_strategy\"],\n",
        "            logging_steps=self.config[\"logging_steps\"],\n",
        "            logging_dir=self.config[\"logging_dir\"],\n",
        "            fp16=self.config[\"fp16\"],\n",
        "            bf16=self.config[\"bf16\"],\n",
        "            push_to_hub=self.config[\"push_to_hub\"],\n",
        "            neftune_noise_alpha=self.config[\"neftune_noise_alpha\"],\n",
        "            report_to=self.config[\"report_to\"]\n",
        "        )\n",
        "\n",
        "    def create_trainer(self):\n",
        "        \"\"\"\n",
        "        Create a trainer for training the model.\n",
        "        \"\"\"\n",
        "        self.load_model_tokenizer()\n",
        "        if self.config[\"use_lora\"]:\n",
        "            print(self.model.print_trainable_parameters())\n",
        "            self.trainer = SFTTrainer(\n",
        "                model=self.model,\n",
        "                train_dataset=self.data,\n",
        "                peft_config=self.lora_config,\n",
        "                dataset_text_field=self.config[\"train_cln_name\"],\n",
        "                args=self.set_training_args(),\n",
        "                tokenizer=self.tokenizer,\n",
        "                packing=self.config[\"packing\"],\n",
        "                max_seq_length=self.config[\"max_seq_length\"]\n",
        "            )\n",
        "        else:\n",
        "            self.trainer = SFTTrainer(\n",
        "                model=self.model,\n",
        "                train_dataset=self.data,\n",
        "                dataset_text_field=self.config[\"train_cln_name\"],\n",
        "                args=self.set_training_args(),\n",
        "                tokenizer=self.tokenizer,\n",
        "                packing=self.config[\"packing\"],\n",
        "                max_seq_length=self.config[\"max_seq_length\"]\n",
        "            )\n",
        "\n",
        "    def train_and_save_model(self):\n",
        "        \"\"\"\n",
        "        Train the model and save it.\n",
        "        \"\"\"\n",
        "        self.create_trainer()\n",
        "        self.trainer.train()\n",
        "        self.trainer.save_model(self.config[\"output_dir\"])\n",
        "        self.tokenizer.save_pretrained(self.config[\"output_dir\"])"
      ],
      "metadata": {
        "id": "0xQoDbB4zNMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7 - Lets start the training process"
      ],
      "metadata": {
        "id": "2N9VTOBGzPTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_sft = TrainSFT(data, sft_config)\n",
        "train_sft.train_and_save_model()"
      ],
      "metadata": {
        "id": "jlaFWJ90zQ5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8 - Merge the model with LoRA weights"
      ],
      "metadata": {
        "id": "TIFy-Q4mzTDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_push_to_hub(config):\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config[\"model_ckpt\"])\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "\n",
        "    # Load base model\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        config[\"model_ckpt\"],\n",
        "        low_cpu_mem_usage=config[\"low_cpu_mem_usage\"],\n",
        "        return_dict=config[\"return_dict\"],\n",
        "        torch_dtype=config[\"torch_dtype\"],\n",
        "        device_map=config[\"device_map\"]\n",
        "    )\n",
        "\n",
        "    # Merge models\n",
        "    merged_model = PeftModel.from_pretrained(base_model,config[\"hub_model_ckpt\"], from_transformers=True)\n",
        "    merged_model = merged_model.merge_and_unload()\n",
        "\n",
        "    # Push the model and tokenizer to the Hugging Face Model Hub\n",
        "    merged_model.push_to_hub(config[\"new_model_ckpt\"], use_temp_dir=False)\n",
        "    tokenizer.push_to_hub(config[\"new_model_ckpt\"], use_temp_dir=False)\n",
        "\n",
        "# Assuming sft_config is defined elsewhere\n",
        "merge_push_to_hub(sft_config)"
      ],
      "metadata": {
        "id": "yOSPVFTRzVYs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}